{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8afa686a-8581-434a-b02e-aff0fb97ea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# XGBoost Classification (XGB) Experiment\n",
    "# ==============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "import xgboost as xgb  # Make sure you have this installed: pip install xgboost\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from preprocessing import prepare_data\n",
    "\n",
    "# Experiment config\n",
    "random_state = 42\n",
    "results = []\n",
    "partial_save_path = \"results/xgb_partial_results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca3722ae-93ab-49d1-a8ae-007e3ad75849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train/test data.\n"
     ]
    }
   ],
   "source": [
    "# ---\n",
    "# 1. Load Data\n",
    "# ---\n",
    "train_df = pd.read_csv(\"fraudTrain.csv\")\n",
    "test_df = pd.read_csv(\"fraudTest.csv\")\n",
    "\n",
    "print(\"Loaded train/test data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76616535-448b-412b-a81c-87fd8de671ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set prepared for trees. Shape: (555719, 13)\n"
     ]
    }
   ],
   "source": [
    "# ---\n",
    "# 2. Prepare Data for TREE Models\n",
    "# ---\n",
    "# We MUST use mode=\"tree\". This skips scaling and uses OrdinalEncoders.\n",
    "out_train_init = prepare_data(\n",
    "    train_df,\n",
    "    mode=\"tree\",\n",
    "    training=False,  # We only need the encoders\n",
    "    fit=True,\n",
    ")\n",
    "encoders = out_train_init[\"encoders\"]\n",
    "scalers = {}  # Scalers are not used\n",
    "\n",
    "# Prepare TEST set using the *same* \"tree\" mode\n",
    "out_test = prepare_data(\n",
    "    test_df,\n",
    "    mode=\"tree\",\n",
    "    training=False,\n",
    "    fit=False,\n",
    "    encoders=encoders,\n",
    "    scalers=scalers,\n",
    ")\n",
    "df_test = out_test[\"df\"]\n",
    "X_test = df_test.drop(\"is_fraud\", axis=1)\n",
    "y_test = df_test[\"is_fraud\"]\n",
    "\n",
    "# Clean inf/-inf values\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(0).clip(-1e6, 1e6)\n",
    "\n",
    "print(f\"Test set prepared for trees. Shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7185884e-3ea8-4160-8c15-34c9dc924927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# 3. Define Experiment Parameters\n",
    "# ---\n",
    "# Based on ALL previous results, we are only testing the best ratios.\n",
    "ratios_to_test = [0.05, 0.1, 0.2] \n",
    "resample_types_to_test = [\"df_up\", \"df_down\"] \n",
    "\n",
    "# XGBoost specific parameters\n",
    "# 'scale_pos_weight' is how XGBoost handles imbalance *internally*.\n",
    "# We will test both our external resampling (what we've been doing)\n",
    "# and its internal method.\n",
    "params_to_test = [\n",
    "    {\n",
    "        \"name\": \"XGB_Resampled\", # Our method\n",
    "        \"params\": {\n",
    "            \"n_estimators\": 200, # More trees\n",
    "            \"max_depth\": 10,       # Based on RF, deep trees are fine\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"objective\": \"binary:logistic\",\n",
    "            \"eval_metric\": \"logloss\",\n",
    "            \"n_jobs\": -1,\n",
    "            \"random_state\": random_state,\n",
    "            \"scale_pos_weight\": 1 # <-- We are handling balance with resampling\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a7735b8-9626-437b-89c3-2277e6c9d8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting XGBoost experiment loop...\n",
      "\n",
      "======================================================================\n",
      "[23:51:22] Starting ratio 1/3 â†’ ratio=0.05\n",
      "  [23:51:26] â†’ Training on df_up (samples=1,353,627)\n",
      "    â³ Running XGB_Resampled ... done â†’ F1=0.8475, Recall=0.7874, AUC=0.9969 | Time=30.4s\n",
      "  [23:51:57] â†’ Training on df_down (samples=157,626)\n",
      "    â³ Running XGB_Resampled ... done â†’ F1=0.7620, Recall=0.8573, AUC=0.9973 | Time=7.2s\n",
      "  ðŸ’¾ Saved intermediate results â†’ results/xgb_partial_results.csv\n",
      "  âœ… Completed ratio=0.05 in 0.7 min\n",
      "\n",
      "======================================================================\n",
      "[23:52:04] Starting ratio 2/3 â†’ ratio=0.1\n",
      "  [23:52:07] â†’ Training on df_up (samples=1,418,086)\n",
      "    â³ Running XGB_Resampled ... done â†’ F1=0.8513, Recall=0.8005, AUC=0.9968 | Time=33.7s\n",
      "  [23:52:41] â†’ Training on df_down (samples=82,566)\n",
      "    â³ Running XGB_Resampled ... done â†’ F1=0.6325, Recall=0.9002, AUC=0.9973 | Time=4.8s\n",
      "  ðŸ’¾ Saved intermediate results â†’ results/xgb_partial_results.csv\n",
      "  âœ… Completed ratio=0.1 in 0.7 min\n",
      "\n",
      "======================================================================\n",
      "[23:52:46] Starting ratio 3/3 â†’ ratio=0.2\n",
      "  [23:52:50] â†’ Training on df_up (samples=1,547,003)\n",
      "    â³ Running XGB_Resampled ... done â†’ F1=0.8460, Recall=0.8065, AUC=0.9971 | Time=28.9s\n",
      "  [23:53:19] â†’ Training on df_down (samples=45,036)\n",
      "    â³ Running XGB_Resampled ... done â†’ F1=0.4999, Recall=0.9343, AUC=0.9972 | Time=4.2s\n",
      "  ðŸ’¾ Saved intermediate results â†’ results/xgb_partial_results.csv\n",
      "  âœ… Completed ratio=0.2 in 0.6 min\n"
     ]
    }
   ],
   "source": [
    "# ---\n",
    "# 4. Run Experiment Loop\n",
    "# ---\n",
    "print(\"\\nStarting XGBoost experiment loop...\")\n",
    "for ratio_idx, ratio in enumerate(ratios_to_test, start=1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\n",
    "        f\"[{datetime.now().strftime('%H:%M:%S')}] Starting ratio {ratio_idx}/{len(ratios_to_test)} â†’ ratio={ratio}\"\n",
    "    )\n",
    "    start_ratio_time = time.time()\n",
    "\n",
    "    out_train = prepare_data(\n",
    "        train_df,\n",
    "        mode=\"tree\",\n",
    "        training=True,\n",
    "        ratio=ratio,\n",
    "        fit=False,  \n",
    "        encoders=encoders,\n",
    "        scalers=scalers,\n",
    "    )\n",
    "\n",
    "    for resample_type in resample_types_to_test:\n",
    "        if resample_type not in out_train or out_train[resample_type] is None:\n",
    "            continue\n",
    "\n",
    "        df_train = out_train[resample_type]\n",
    "        X_train = df_train.drop(\"is_fraud\", axis=1)\n",
    "        y_train = df_train[\"is_fraud\"]\n",
    "\n",
    "        X_train = X_train.replace([np.inf, -np.inf], np.nan).fillna(0).clip(-1e6, 1e6)\n",
    "\n",
    "        print(\n",
    "            f\"  [{datetime.now().strftime('%H:%M:%S')}] â†’ Training on {resample_type} (samples={len(X_train):,})\"\n",
    "        )\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        for p_info in params_to_test:\n",
    "            model_name = p_info[\"name\"]\n",
    "            params = p_info[\"params\"]\n",
    "            start_k_time = time.time()\n",
    "\n",
    "            print(f\"    â³ Running {model_name} ...\", end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            model = xgb.XGBClassifier(**params)\n",
    "            \n",
    "            model.fit(\n",
    "                X_train, \n",
    "                y_train,\n",
    "                eval_set=[(X_test, y_test)],\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            # Predict\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            # Metrics\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            rec = recall_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_prob)\n",
    "            prec = precision_score(y_test, y_pred)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "            # === FIX #1: Handle AttributeError ===\n",
    "            try:\n",
    "                best_iter = model.best_iteration\n",
    "            except AttributeError:\n",
    "                best_iter = model.n_estimators # Set to full run if no early stop\n",
    "            # =====================================\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"model\": model_name,\n",
    "                    \"ratio\": ratio,\n",
    "                    \"resample_type\": resample_type.replace(\"df_\",\"\"),\n",
    "                    \"accuracy\": acc,\n",
    "                    \"precision\": prec,\n",
    "                    \"recall\": rec,\n",
    "                    \"f1\": f1,\n",
    "                    \"roc_auc\": auc,\n",
    "                    \"best_iter\": best_iter\n",
    "                }\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\" done â†’ F1={f1:.4f}, Recall={rec:.4f}, AUC={auc:.4f} | Time={time.time() - start_k_time:.1f}s\"\n",
    "            )\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    pd.DataFrame(results).to_csv(partial_save_path, index=False)\n",
    "    print(f\"  ðŸ’¾ Saved intermediate results â†’ {partial_save_path}\")\n",
    "    print(f\"  âœ… Completed ratio={ratio} in {(time.time() - start_ratio_time)/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e157339e-a374-4fc8-9b56-512cb8bfbe4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "[23:54:22] Starting Internal Imbalance Handling (scale_pos_weight)\n",
      "  Using original training data (samples=1,296,675)\n",
      "  Calculated scale_pos_weight: 171.75 (Legit: 1289169, Fraud: 7506)\n",
      "    â³ Running XGB_Internal_ScalePos ... done â†’ F1=0.8069, Recall=0.8065, AUC=0.9959 | Time=22.9s\n",
      "  ðŸ’¾ Saved intermediate results â†’ results/xgb_partial_results.csv\n",
      "  âœ… Completed internal run in 0.4 min\n"
     ]
    }
   ],
   "source": [
    "# ---\n",
    "# 5. Add the \"Internal Resampling\" Experiment (API FIX)\n",
    "# ---\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Starting Internal Imbalance Handling (scale_pos_weight)\")\n",
    "start_ratio_time = time.time()\n",
    "\n",
    "out_train_orig = prepare_data(\n",
    "    train_df,\n",
    "    mode=\"tree\",\n",
    "    training=True,\n",
    "    ratio=None, \n",
    "    fit=False,  \n",
    "    encoders=encoders,\n",
    "    scalers=scalers,\n",
    ")\n",
    "\n",
    "df_train_orig = out_train_orig[\"df_up\"] \n",
    "X_train_orig = df_train_orig.drop(\"is_fraud\", axis=1)\n",
    "y_train_orig = df_train_orig[\"is_fraud\"]\n",
    "\n",
    "X_train_orig = X_train_orig.replace([np.inf, -np.inf], np.nan).fillna(0).clip(-1e6, 1e6)\n",
    "\n",
    "n_legit = (y_train_orig == 0).sum()\n",
    "n_fraud = (y_train_orig == 1).sum()\n",
    "imbalance_ratio = n_legit / n_fraud\n",
    "print(f\"  Using original training data (samples={len(X_train_orig):,})\")\n",
    "print(f\"  Calculated scale_pos_weight: {imbalance_ratio:.2f} (Legit: {n_legit}, Fraud: {n_fraud})\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "model_name = \"XGB_Internal_ScalePos\"\n",
    "# === API FIX: MOVED PARAMS HERE ===\n",
    "params = {\n",
    "    \"n_estimators\": 200,\n",
    "    \"max_depth\": 10,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": random_state,\n",
    "    \"scale_pos_weight\": imbalance_ratio,\n",
    "    \"early_stopping_rounds\": 20,\n",
    "    \"eval_metric\": \"logloss\"\n",
    "}\n",
    "\n",
    "start_k_time = time.time()\n",
    "print(f\"    â³ Running {model_name} ...\", end=\"\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "model = xgb.XGBClassifier(**params)\n",
    "# === API FIX: Removed early_stopping_rounds from .fit() ===\n",
    "model.fit(\n",
    "    X_train_orig, \n",
    "    y_train_orig,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Predict & Metrics\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "results.append(\n",
    "    {\n",
    "        \"model\": model_name,\n",
    "        \"ratio\": \"internal\", \n",
    "        \"resample_type\": \"none\",\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "        \"roc_auc\": auc,\n",
    "        # === API FIX: Use .best_iteration attribute ===\n",
    "        \"best_iter\": model.best_iteration\n",
    "    }\n",
    ")\n",
    "print(\n",
    "    f\" done â†’ F1={f1:.4f}, Recall={rec:.4f}, AUC={auc:.4f} | Time={time.time() - start_k_time:.1f}s\"\n",
    ")\n",
    "print(f\"  ðŸ’¾ Saved intermediate results â†’ {partial_save_path}\")\n",
    "print(f\"  âœ… Completed internal run in {(time.time() - start_ratio_time)/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23025a1e-5beb-496a-915d-bbf0567adad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All ratios completed.\n",
      "Total experiments logged: 7\n",
      "\n",
      "--- Top Performing XGBoost Models ---\n",
      "                   model     ratio resample_type  accuracy  precision  \\\n",
      "2          XGB_Resampled       0.1            up  0.998920   0.908947   \n",
      "0          XGB_Resampled      0.05            up  0.998906   0.917436   \n",
      "4          XGB_Resampled       0.2            up  0.998866   0.889460   \n",
      "6  XGB_Internal_ScalePos  internal          none  0.998510   0.807280   \n",
      "1          XGB_Resampled      0.05          down  0.997932   0.685682   \n",
      "3          XGB_Resampled       0.1          down  0.995962   0.487503   \n",
      "5          XGB_Resampled       0.2          down  0.992784   0.341223   \n",
      "\n",
      "     recall        f1   roc_auc  best_iter  \n",
      "2  0.800466  0.851264  0.996830        200  \n",
      "0  0.787413  0.847466  0.996908        200  \n",
      "4  0.806527  0.845966  0.997123        200  \n",
      "6  0.806527  0.806903  0.995876        198  \n",
      "1  0.857343  0.761964  0.997330        200  \n",
      "3  0.900233  0.632493  0.997302        200  \n",
      "5  0.934266  0.499875  0.997176        200  \n",
      "\n",
      "Saved final results to results/xgb_results.csv\n"
     ]
    }
   ],
   "source": [
    "# ---\n",
    "# 6. Show Final Results\n",
    "# ---\n",
    "print(\"\\nAll ratios completed.\")\n",
    "print(f\"Total experiments logged: {len(results)}\")\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"f1\", ascending=False)\n",
    "print(\"\\n--- Top Performing XGBoost Models ---\")\n",
    "print(results_df.head(10))\n",
    "\n",
    "results_df.to_csv(\"results/xgb_results.csv\", index=False)\n",
    "print(\"\\nSaved final results to results/xgb_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34afb3e-f100-4e8c-81cd-1f3e0fb60fba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
